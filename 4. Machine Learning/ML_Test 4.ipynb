{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8d58d0c",
   "metadata": {},
   "source": [
    "## Algorithm questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2eb43ea",
   "metadata": {},
   "source": [
    "### 1.How does regularization (L1 and L2) help in preventing overfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f417abd4",
   "metadata": {},
   "source": [
    "Regularization is a crucial technique that helps prevent overfitting, which occurs when a model learns the training data too well, including its noise and outliers, resulting in poor performance on unseen data.\n",
    "Two common forms of regularization are L1 regularization (Lasso) and L2 regularization (Ridge), each employing different strategies to achieve this goal.\n",
    "\n",
    "L1 Regularization (Lasso)\n",
    "Mechanism: L1 regularization adds a penalty equal to the absolute value of the magnitude of coefficients. This can shrink some coefficients to exactly zero, effectively performing feature selection.\n",
    "\n",
    "L2 Regularization (Ridge)\n",
    "Mechanism: L2 regularization adds a penalty equal to the square of the magnitude of coefficients. Unlike L1, it does not set coefficients to zero but rather shrinks them uniformly.\n",
    "\n",
    "Preventing Overfitting-\n",
    "Both L1 and L2 regularization help in preventing overfitting through:\n",
    "\n",
    "Control of Model Complexity: By penalizing large weights, these techniques force the model to focus on the most significant patterns in the data rather than memorizing noise.\n",
    "\n",
    "Improved Generalization: Regularized models tend to perform better on unseen data because they are less likely to capture idiosyncrasies specific to the training set. This leads to better predictive performance across different datasets.\n",
    "\n",
    "Balancing Fit and Complexity: The choice of λ is critical, if it's too high, it may lead to underfitting (a model is too simple), while if it's too low, it may not sufficiently reduce overfitting. Thus, finding an optimal balance is essential for effective modeling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c302f46",
   "metadata": {},
   "source": [
    "### 2.Why is feature scaling important in gradient descent?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d596b41c",
   "metadata": {},
   "source": [
    "Importance of Feature Scaling in Gradient Descent-\n",
    "\n",
    "Uniform Step Sizes: Gradient descent updates model parameters based on the gradients of the loss function with respect to each feature. If features are on different scales, the gradients will also vary significantly, leading to inconsistent step sizes during updates. This can cause the algorithm to converge slowly or even diverge if one feature dominates due to its larger scale.\n",
    "\n",
    "Faster Convergence: By scaling features to a similar range e.g., 0 to 1, the optimization process becomes more efficient. This uniformity allows gradient descent to make more consistent progress towards the minima of the loss function, thus speeding up convergence. \n",
    "\n",
    "Avoiding Local Minima: When features are not scaled, the cost surface can become elongated or skewed, making it difficult for the gradient descent algorithm to navigate effectively. This can increase the likelihood of getting stuck in local minima rather than finding the global minimum.\n",
    "\n",
    "Improved Model Performance: Many machine learning models assume that input features are centered around zero and have similar variances. Without scaling, features with larger magnitudes can disproportionately influence model training, leading to biased results.\n",
    "\n",
    "Enhanced Interpretability: In models like linear regression, scaling helps interpret coefficients more effectively since all features are on a common scale. This makes it easier to assess the relative importance of each feature in predicting the target variable.\n",
    "\n",
    "Compatibility with Distance-Based Algorithms: While not directly related to gradient descent, it's worth noting that algorithms relying on distance metrics (e.g., k-NN, SVM) also benefit from feature scaling. In these cases, unscaled features can lead to misleading distance calculations, further emphasizing the need for scaling in any comprehensive machine learning workflow.\n",
    "\n",
    "Feature scaling is essential for ensuring efficient and effective training of models using gradient descent by promoting uniformity in parameter updates, accelerating convergence, and enhancing overall model performance and interpretability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88785272",
   "metadata": {},
   "source": [
    "## Problem Solving"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c904fab",
   "metadata": {},
   "source": [
    "### 1.Given a dataset with missing values, how would you handle them before training an ML model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be887c2",
   "metadata": {},
   "source": [
    "Handling Missing Values:\n",
    "\n",
    "Remove Missing Data-\n",
    "\n",
    "Delete Rows: If only a few rows have missing values, you can remove those rows entirely. This is effective if the loss of data does not significantly affect the dataset.\n",
    "\n",
    "Delete Columns: If a column has too many missing values (e.g., more than 50%), consider dropping the entire column since it may not provide useful information.\n",
    "\n",
    "Imputation-\n",
    "\n",
    "Mean/Median/Mode Imputation: For numerical features, replace missing values with the mean or median of that feature. For categorical features, use the mode (most frequent value).\n",
    "\n",
    "Interpolation: Use interpolation methods to estimate missing values based on existing data points. This is particularly useful for time-series data.\n",
    "\n",
    "Predictive Imputation: Use other features in the dataset to predict and fill in missing values using machine learning models.\n",
    "\n",
    "Using Algorithms that Support Missing Values: Some algorithms can handle missing values directly without requiring imputation (e.g., certain tree-based models).\n",
    "\n",
    "Evaluate Impact: Always assess how our chosen method affects the dataset and the model's performance, as improper handling can introduce bias."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37450cba",
   "metadata": {},
   "source": [
    "### 2.Design a pipeline for building a classification model. Include steps for data preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d291cbfb",
   "metadata": {},
   "source": [
    "Steps in the Pipeline\n",
    "\n",
    "Data Collection: Gather your dataset from reliable sources.\n",
    "\n",
    "Data Preprocessing:\n",
    "Handle Missing Values: Use Mean, Median, Mode to deal with any missing data.\n",
    "\n",
    "Feature Scaling: Normalize or standardize your features so they are on similar scales, which helps improve model performance.\n",
    "\n",
    "Encoding Categorical Variables: Convert categorical variables into numerical format using techniques like one-hot encoding or label encoding.\n",
    "\n",
    "Split the Dataset:\n",
    "Divide your dataset into training and testing sets (70% - 30%) to evaluate model performance on unseen data.\n",
    "\n",
    "Model Selection:\n",
    "Choose an appropriate classification algorithm (e.g., Logistic Regression, Decision Trees, Random Forests).\n",
    "\n",
    "Model Training:\n",
    "Fit the model to the training data using selected features and labels.\n",
    "\n",
    "Model Evaluation:\n",
    "Test the model on the testing set to evaluate its performance using metrics such as accuracy, precision, recall, and F1-score.\n",
    "\n",
    "Hyperparameter Tuning:\n",
    "Optimize model parameters using techniques like grid search or random search to find the best configuration.\n",
    "\n",
    "Final Model Deployment:\n",
    "Once satisfied with the model's performance, deploy it for predictions on new data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229f9965",
   "metadata": {},
   "source": [
    "## Coding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5865fb1a",
   "metadata": {},
   "source": [
    "### 1.Write a Python script to implement a decision tree classifier using Scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9083628e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        10\n",
      "           1       1.00      1.00      1.00         9\n",
      "           2       1.00      1.00      1.00        11\n",
      "\n",
      "    accuracy                           1.00        30\n",
      "   macro avg       1.00      1.00      1.00        30\n",
      "weighted avg       1.00      1.00      1.00        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Load a sample dataset (Iris dataset)\n",
    "data = load_iris()\n",
    "X = data.data  \n",
    "y = data.target  \n",
    "\n",
    "\n",
    "# Split the dataset into training and testing sets (80-20 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "clf = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bd502b",
   "metadata": {},
   "source": [
    "### 2.Given a dataset, write code to split the data into training and testing sets using an 80-20 split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e2182f53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Dataset:\n",
      "   Pclass     Sex  Age  SibSp  Parch     Fare  Survived\n",
      "0       1    male   22      1      0  71.2833         1\n",
      "1       1  female   38      1      0  53.1000         1\n",
      "2       3  female   26      0      0   8.0500         0\n",
      "3       1  female   35      0      0   8.0500         1\n",
      "4       3    male   35      0      0   8.0500         0\n",
      "5       2    male   28      0      1  13.0000         0\n",
      "6       3  female   14      1      0   7.2250         1\n",
      "7       2    male   40      0      0   8.0500         0\n",
      "\n",
      "Training set shape: (6, 6)\n",
      "Testing set shape: (2, 6)\n",
      "\n",
      "Training Features:\n",
      "   Pclass     Sex  Age  SibSp  Parch     Fare\n",
      "0       1    male   22      1      0  71.2833\n",
      "7       2    male   40      0      0   8.0500\n",
      "2       3  female   26      0      0   8.0500\n",
      "4       3    male   35      0      0   8.0500\n",
      "3       1  female   35      0      0   8.0500\n",
      "6       3  female   14      1      0   7.2250\n",
      "\n",
      "Testing Features:\n",
      "   Pclass     Sex  Age  SibSp  Parch  Fare\n",
      "1       1  female   38      1      0  53.1\n",
      "5       2    male   28      0      1  13.0\n",
      "\n",
      "Training Target:\n",
      "0    1\n",
      "7    0\n",
      "2    0\n",
      "4    0\n",
      "3    1\n",
      "6    1\n",
      "Name: Survived, dtype: int64\n",
      "\n",
      "Testing Target:\n",
      "1    1\n",
      "5    0\n",
      "Name: Survived, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Create a simple synthetic Titanic dataset\n",
    "data = {\n",
    "    'Pclass': [1, 1, 3, 1, 3, 2, 3, 2],\n",
    "    'Sex': ['male', 'female', 'female', 'female', 'male', 'male', 'female', 'male'],\n",
    "    'Age': [22, 38, 26, 35, 35, 28, 14, 40],\n",
    "    'SibSp': [1, 1, 0, 0, 0, 0, 1, 0],\n",
    "    'Parch': [0, 0, 0, 0, 0, 1, 0, 0],\n",
    "    'Fare': [71.2833, 53.1000, 8.0500, 8.0500, 8.0500, 13.0000, 7.2250, 8.0500],\n",
    "    'Survived': [1, 1, 0, 1, 0, 0, 1, 0]\n",
    "}\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the original dataset\n",
    "print(\"Original Dataset:\")\n",
    "print(df)\n",
    "\n",
    "# Define features (X) and target variable (y)\n",
    "X = df.drop(columns=['Survived'])   # Features\n",
    "y = df['Survived']                   # Target variable\n",
    "\n",
    "# Split the dataset into training and testing sets (80-20 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Display the shapes of the resulting datasets\n",
    "print(\"\\nTraining set shape:\", X_train.shape)\n",
    "print(\"Testing set shape:\", X_test.shape)\n",
    "\n",
    "# Display the training and testing sets\n",
    "print(\"\\nTraining Features:\")\n",
    "print(X_train)\n",
    "print(\"\\nTesting Features:\")\n",
    "print(X_test)\n",
    "print(\"\\nTraining Target:\")\n",
    "print(y_train)\n",
    "print(\"\\nTesting Target:\")\n",
    "print(y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac9ccaa",
   "metadata": {},
   "source": [
    "## Case Study"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4130c0",
   "metadata": {},
   "source": [
    "### A company wants to predict employee attrition. What kind of ML problem is this? Which algorithms would you choose and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b28e398",
   "metadata": {},
   "source": [
    "Predicting employee attrition is a supervised classification problem in machine learning. The goal is to determine whether an employee is likely to leave the company (attrition) based on various features such as job satisfaction, salary, performance ratings, and other demographic or employment-related factors.\n",
    "\n",
    "Recommended Algorithms-\n",
    "\n",
    "Logistic Regression:\n",
    "\n",
    "Reason: Logistic regression is a simple and interpretable model that works well for binary classification problems. It estimates the probability that an employee will leave based on input features. It is particularly useful when you want to understand the impact of individual features on the likelihood of attrition.\n",
    "\n",
    "Random Forest:\n",
    "\n",
    "Reason: Random Forest is an ensemble method that combines multiple decision trees to improve accuracy and reduce overfitting. It is robust against noise and can handle large datasets with higher dimensionality effectively.\n",
    "\n",
    "Gradient Boosting Machines (e.g., XGBoost):\n",
    "\n",
    "Reason: XGBoost is known for its high performance in classification tasks due to its ability to handle missing values, regularization, and parallel processing. It often yields better accuracy than traditional models by focusing on correcting errors made by previous models in the ensemble.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85c3cf8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
